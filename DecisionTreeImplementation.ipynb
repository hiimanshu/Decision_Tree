{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = datasets.load_iris()\n",
    "X = df.data\n",
    "Y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeLabelled(column):\n",
    "    \"\"\" Marks a label to each row of a column \"\"\"\n",
    "    second_limit = column.mean()\n",
    "    first_limit = 0.5*second_limit\n",
    "    third_limit = 1.5*second_limit\n",
    "    for i in range(0,len(column)):\n",
    "        if(column[i] < first_limit):\n",
    "            column[i] = 0\n",
    "        elif (column[i] < second_limit):\n",
    "            column[i] = 1\n",
    "        elif (column[i] < third_limit):\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_unique_values(x, col):\n",
    "    \"\"\" Returns unique values of a column of X data set\"\"\"\n",
    "    return set([row[col] for row in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_entropy(y):\n",
    "    \"\"\" Returns entropy of the current classes \"\"\"\n",
    "    dictionary = {}  # No of data points of each class is stored in a dictionary\n",
    "    entropy = 0\n",
    "    s = set(y)\n",
    "    for i in s:\n",
    "        dictionary[i] = (y == i).sum()\n",
    "    for key in dictionary:\n",
    "        prob = dictionary[key]/len(y) # probability of each class\n",
    "        if(prob != 0):\n",
    "            entropy += (-1)*prob*log(prob,2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_info_gain(x,y,f):\n",
    "    \"\"\" Returns Information gain on the basis of the given feature \"\"\"\n",
    "    parent_entropy = find_entropy(y)  # Entropy of parent node\n",
    "    s = find_unique_values(x,f)\n",
    "    result = {}      # result will store the entropy of each child\n",
    "    dictionary = {}  # dictionary stores no of data points of each unique value of x[:,f]\n",
    "    for e in s:\n",
    "        rows = (x[:,f] == e)\n",
    "        dictionary[e] = rows.sum()   # it will be used to find the weighted avg of children entropy\n",
    "        result[e] = find_entropy(y[rows])\n",
    "    weighted_child_entropy = 0      # this will store the weighted children entropy\n",
    "    for key in result:\n",
    "        weighted_child_entropy += (dictionary[key]/len(x))*(result[key])\n",
    "    return parent_entropy - weighted_child_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_case_pure_node(y,level):\n",
    "    \"\"\" Base case for a pure node \"\"\"\n",
    "    print(\"Level:\", level)\n",
    "    print(\"Count of\", y[0], \"=\", len(y))\n",
    "    print(\"Current Entropy is = 0.0\")\n",
    "    print(\"Leaf Node Reached\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_case_no_features_left(y,level):\n",
    "    \"\"\" Base case when no feature are left to split \"\"\"\n",
    "    print(\"Level:\", level)\n",
    "    s = set(y)\n",
    "    for e in s:\n",
    "        print(\"Count of\", e, \"=\", (y == e).sum())\n",
    "    print(\"Current Entropy is\", find_entropy(y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_current_node(y,max_gain,final_feature,level):\n",
    "    \"\"\" Prints status of current node \"\"\"\n",
    "    print(\"Level:\", level)\n",
    "    s = set(y)\n",
    "    for e in s:\n",
    "        print(\"Count of\", e, \"=\", (y == e).sum())\n",
    "    print(\"Current Entropy is\", find_entropy(y))\n",
    "    print(\"Splitting on feature\", final_feature, \" with information gain\", max_gain)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_split(x,y,features):\n",
    "    \"\"\" Finds best split for the dataset on the basis of information gain\n",
    "        Returns the maximum info_gain and the feature on which to split\n",
    "    \"\"\"\n",
    "    max_gain = 0\n",
    "    final_feature = -2 # assigned a random value which the final_feature cannot take\n",
    "    for f in features:\n",
    "        info_gain = find_info_gain(x,y,f)\n",
    "        if(info_gain > max_gain):     # compares info_gain of each feature of x\n",
    "            max_gain = info_gain\n",
    "            final_feature = f\n",
    "    return max_gain, final_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(x,y,features,final_feature,level):\n",
    "    \"\"\" Partition the dataset on the basis of feature split which has highest information gain \"\"\"\n",
    "    s = set(x[:,final_feature]) # Returns unique values of x for final_feature column\n",
    "    features.remove(final_feature) # Deleted the feature on which the split has to be made\n",
    "    for i in s:\n",
    "        arr = (x[:,final_feature] == i)\n",
    "        build_DT(x[arr], y[arr], features, level + 1)   # RECURSIVE CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DT(x, y, features, level):\n",
    "    \"\"\" This function builds the decision tree\"\"\"\n",
    "    #base case for pure node\n",
    "    if (len(set(y)) == 1):\n",
    "        base_case_pure_node(y, level)\n",
    "        return\n",
    "    \n",
    "    #base case when all features are used in splitting\n",
    "    if(len(features) == 0):\n",
    "        base_case_no_features_left(y, level)\n",
    "        return\n",
    "    \n",
    "    #Decision to make on which feature to split on\n",
    "    max_gain, final_feature = find_best_split(x, y, features)\n",
    "    \n",
    "    # Printing current node\n",
    "    print_current_node(y, max_gain, final_feature, level)\n",
    "    \n",
    "    #Splitting the data on the basis of maximum gain\n",
    "    partition(x, y, features, final_feature, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: 0\n",
      "Count of 0 = 50\n",
      "Count of 1 = 50\n",
      "Count of 2 = 50\n",
      "Current Entropy is 1.58496250072\n",
      "Splitting on feature 3  with information gain 1.35656522266\n",
      "\n",
      "Level: 1\n",
      "Count of 0 = 49\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Level: 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 10\n",
      "Current Entropy is 0.439496986922\n",
      "Splitting on feature 1  with information gain 0.439496986922\n",
      "\n",
      "Level: 2\n",
      "Count of 1 = 10\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Level: 2\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Level: 1\n",
      "Count of 1 = 39\n",
      "Count of 2 = 5\n",
      "Current Entropy is 0.510787822954\n",
      "Splitting on feature 2  with information gain 0.0776949537301\n",
      "\n",
      "Level: 2\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Level: 2\n",
      "Count of 1 = 38\n",
      "Count of 2 = 4\n",
      "Current Entropy is 0.453716339187\n",
      "Splitting on feature 0  with information gain 0.00399336146638\n",
      "\n",
      "Level: 3\n",
      "Count of 1 = 14\n",
      "Count of 2 = 1\n",
      "Current Entropy is 0.353359335021\n",
      "\n",
      "Level: 3\n",
      "Count of 1 = 24\n",
      "Count of 2 = 3\n",
      "Current Entropy is 0.503258334776\n",
      "\n",
      "Level: 2\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Level: 1\n",
      "Count of 1 = 1\n",
      "Count of 2 = 45\n",
      "Current Entropy is 0.151096970517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_DT(X,Y,features,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
