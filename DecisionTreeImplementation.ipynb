{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = datasets.load_iris()\n",
    "X = df.data\n",
    "Y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeLabelled(column):\n",
    "    second_limit = column.mean()\n",
    "    first_limit = 0.5*second_limit\n",
    "    third_limit = 1.5*second_limit\n",
    "    for i in range(0,len(column)):\n",
    "        if(column[i] < first_limit):\n",
    "            column[i] = 0\n",
    "        elif (column[i] < second_limit):\n",
    "            column[i] = 1\n",
    "        elif (column[i] < third_limit):\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_unique_values(x, col):\n",
    "    return set([row[col] for row in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_entropy(y):\n",
    "    dictionary = {}\n",
    "    entropy = 0\n",
    "    s = set(y)\n",
    "    for e in s:\n",
    "        dictionary[e] = (y == e).sum()\n",
    "    for key in dictionary:\n",
    "        prob = dictionary[key]/len(y)\n",
    "        if(prob != 0):\n",
    "            entropy += (-1)*prob*log(prob,2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_info_gain(x,y,f):\n",
    "    parent_entropy = find_entropy(y)\n",
    "    s = find_unique_values(x,f)\n",
    "    result = {}\n",
    "    dictionary = {}\n",
    "    for e in s:\n",
    "        rows = (x[:,f] == e)\n",
    "        dictionary[e] = rows.sum()\n",
    "        result[e] = find_entropy(y[rows])\n",
    "    weighted_child_entropy = 0\n",
    "    for key in result:\n",
    "        weighted_child_entropy += (dictionary[key]/len(x))*(result[key])\n",
    "    return parent_entropy - weighted_child_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_case_pure_node(y):\n",
    "    print(\"Base Case for pure node:-\")\n",
    "    print(\"Count of\",y[0],\"=\",len(y))\n",
    "    print(\"Current Entropy is = 0.0\")\n",
    "    print(\"Leaf Node Reached\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_case_no_features_left(y):\n",
    "    print(\"Base case when no features are present:-\")\n",
    "    s = set(y)\n",
    "    for e in s:\n",
    "        print(\"Count of \",e,\" =\",(y == e).sum())\n",
    "    print(\"Current Entropy is \",find_entropy(y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_split(x,y,features):\n",
    "    max_gain = -1000\n",
    "    final_feature = -1\n",
    "    for f in features:\n",
    "        info_gain = find_info_gain(x,y,f)\n",
    "        if(info_gain > max_gain):\n",
    "            max_gain = info_gain\n",
    "            final_feature = f\n",
    "    return max_gain, final_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_current_node(y,max_gain,final_feature):\n",
    "    print(\"Normal Node:-\")\n",
    "    s = set(y)\n",
    "    for e in s:\n",
    "        print(\"Count of\",e,\"=\",(y == e).sum())\n",
    "    print(\"Current Entropy is \",find_entropy(y))\n",
    "    print(\"Splitting on feature \",final_feature,\" with information gain \",max_gain)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition(x,y,features,final_feature):\n",
    "    s = set(x[:,final_feature])\n",
    "    features.remove(final_feature)\n",
    "    for e in s:\n",
    "        arr = (x[:,final_feature] == e)\n",
    "        build_DT(x[arr], y[arr], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DT(x, y, features, level):\n",
    "    #base case for pure node\n",
    "    if (len(set(y)) == 1):\n",
    "        base_case_pure_node(y)\n",
    "        return\n",
    "    #base case when all features are used in splitting\n",
    "    if(len(features) == 0):\n",
    "        base_case_no_features_left(y)\n",
    "        return\n",
    "    #Decision to make on which feature to split on\n",
    "    max_gain, final_feature = find_best_split(x,y,features)\n",
    "    # Printing current node\n",
    "    print_current_node(y,max_gain,final_feature)\n",
    "    #Splitting the data on the basis of maximum gain\n",
    "    partition(x,y,features,final_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array([\n",
    "    [0, 3, 0],\n",
    "    [1, 3, 0],\n",
    "    [2, 1, 1],\n",
    "    [2, 1, 1],\n",
    "    [1, 3, 2],\n",
    "])\n",
    "x = np.array(training_data[:,:2])\n",
    "y = np.array(training_data[:,-1])\n",
    "features = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Node:-\n",
      "Count of 0 = 50\n",
      "Count of 1 = 50\n",
      "Count of 2 = 50\n",
      "Current Entropy is  1.58496250072\n",
      "Splitting on feature  3  with information gain  1.35656522266\n",
      "\n",
      "Base Case for pure node:-\n",
      "Count of 0 = 49\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Normal Node:-\n",
      "Count of 0 = 1\n",
      "Count of 1 = 10\n",
      "Current Entropy is  0.439496986922\n",
      "Splitting on feature  1  with information gain  0.439496986922\n",
      "\n",
      "Base Case for pure node:-\n",
      "Count of 1 = 10\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Base Case for pure node:-\n",
      "Count of 0 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Normal Node:-\n",
      "Count of 1 = 39\n",
      "Count of 2 = 5\n",
      "Current Entropy is  0.510787822954\n",
      "Splitting on feature  2  with information gain  0.0776949537301\n",
      "\n",
      "Base Case for pure node:-\n",
      "Count of 1 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Normal Node:-\n",
      "Count of 1 = 38\n",
      "Count of 2 = 4\n",
      "Current Entropy is  0.453716339187\n",
      "Splitting on feature  0  with information gain  0.00399336146638\n",
      "\n",
      "Base case when no features are present:-\n",
      "Count of  1  = 14\n",
      "Count of  2  = 1\n",
      "Current Entropy is  0.353359335021\n",
      "\n",
      "Base case when no features are present:-\n",
      "Count of  1  = 24\n",
      "Count of  2  = 3\n",
      "Current Entropy is  0.503258334776\n",
      "\n",
      "Base Case for pure node:-\n",
      "Count of 2 = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node Reached\n",
      "\n",
      "Base case when no features are present:-\n",
      "Count of  1  = 1\n",
      "Count of  2  = 45\n",
      "Current Entropy is  0.151096970517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_DT(X,Y,features,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
